
# Neural Network vs Logistic Regression â€” Iris Classification

## ğŸ“Œ Project Overview
This project implements a **feedforward Neural Network** from scratch (using PyTorch) to perform **multiclass classification** on the classic **Iris dataset**, and compares its performance against a strong linear baseline: **Logistic Regression**.

The goal is not to â€œbeatâ€ traditional models blindly, but to **understand when and why Neural Networks help**, and when simpler models are equally effective.

---

## ğŸ¯ Problem Statement
Given four flower measurements:
- Sepal length
- Sepal width
- Petal length
- Petal width  

Predict the flower species:
- Setosa  
- Versicolor  
- Virginica  

This is a **3-class classification problem**.

---

## ğŸ§  Models Implemented

### 1ï¸âƒ£ Neural Network (PyTorch)
A fully connected feedforward neural network trained using a **manual training loop**.

**Architecture**
```
Input (4 features)
â†“
Dense (16) + ReLU
â†“
Dense (16) + ReLU
â†“
Dense (3 logits)
```

- Loss Function: CrossEntropyLoss  
- Optimizer: Adam  
- Feature Scaling: StandardScaler  
- Labels: Encoded using LabelEncoder  

> Note: Softmax is **not applied explicitly**, as `CrossEntropyLoss` expects raw logits.

---

### 2ï¸âƒ£ Logistic Regression (Baseline)
A classic linear classifier implemented using **scikit-learn**, serving as a strong baseline for comparison.

- Multinomial Logistic Regression  
- Max iterations: 200  

---

## ğŸ“Š Results

| Model               | Accuracy |
|--------------------|----------|
| Neural Network     | **0.9333** |
| Logistic Regression| **0.9333** |

### Interpretation
- The Iris dataset is **nearly linearly separable**
- Logistic Regression performs extremely well
- The Neural Network matches the baseline, demonstrating **correct learning and generalization**
- This highlights an important ML principle:

> **More complex models do not always outperform simpler ones on well-structured data.**

---

## ğŸ“ˆ Visualizations Included

- **Neural Network training loss vs epochs**
- **Confusion Matrix** for Neural Network
- **Confusion Matrix** for Logistic Regression
- **PCA projection** of the Iris dataset (2D visualization of class separability)

These plots help validate both **learning behavior** and **model performance**.

---

## ğŸ—‚ï¸ Project Structure

```
nn-iris-classification/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ iris.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py          # Neural Network architecture
â”‚   â”œâ”€â”€ train_nn.py       # NN training loop
â”‚   â”œâ”€â”€ train_logr.py     # Logistic Regression baseline
â”‚   â”œâ”€â”€ evaluate.py       # Evaluation utilities
â”‚
â”œâ”€â”€ visualizations/
â”‚   â””â”€â”€ visualize.py      # Loss curves, confusion matrices, PCA
â”‚
â”œâ”€â”€ main.py               # Pipeline orchestration
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Key ML Concepts Demonstrated

- Multiclass classification
- Neural Networks vs linear models
- Feature scaling importance
- Label encoding for deep learning
- Manual PyTorch training loop
- Fair baseline comparison
- Model evaluation beyond accuracy
- Visualization-driven validation

---

## ğŸ§  Key Takeaway
This project demonstrates that **Neural Networks are powerful function approximators**, but also reinforces a core machine learning lesson:

> *Model selection should be driven by data characteristics, not model complexity.*

---

## ğŸš€ Possible Extensions
- Decision boundary visualization (2-feature subspace)
- Deeper neural networks
- Hyperparameter tuning
- Classification report (precision, recall, F1)
- Comparison with SVM or Random Forest

---

## ğŸ› ï¸ Tech Stack
- Python
- PyTorch
- scikit-learn
- NumPy
- pandas
- matplotlib

---

## âœ… Status
âœ” Complete  
âœ” Reproducible  
âœ” Portfolio-ready  

---

*Built to understand, not just to score.*
